{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6N+SPCESyY9RNZLmlDsvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1bb44259db0048ef92da5449811c3833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edb685fe518048e5ad0c9705959945f0",
              "IPY_MODEL_5ab6c34240df40909ffc389d66e0f867",
              "IPY_MODEL_81d78fb3f6174040866fe270fb78c4d7"
            ],
            "layout": "IPY_MODEL_25af7f4a4a5144d59c03e6e9ba2ce780"
          }
        },
        "edb685fe518048e5ad0c9705959945f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d82ffef4d134a1fa5b9f3dffd20c64f",
            "placeholder": "​",
            "style": "IPY_MODEL_175fa2f24d41419eab80d53bdd32a007",
            "value": "  0%"
          }
        },
        "5ab6c34240df40909ffc389d66e0f867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b14271c87afe4b6a98f70a366cce8ac3",
            "max": 40461,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73a1ee18e33a4023a067bc1feb7d7613",
            "value": 0
          }
        },
        "81d78fb3f6174040866fe270fb78c4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818a3ac364a447dd8c955532f01cf9d0",
            "placeholder": "​",
            "style": "IPY_MODEL_099e9f2bc9c647d8a41f33c4632e3f00",
            "value": " 0/40461 [00:00&lt;?, ?it/s]"
          }
        },
        "25af7f4a4a5144d59c03e6e9ba2ce780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d82ffef4d134a1fa5b9f3dffd20c64f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "175fa2f24d41419eab80d53bdd32a007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b14271c87afe4b6a98f70a366cce8ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a1ee18e33a4023a067bc1feb7d7613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "818a3ac364a447dd8c955532f01cf9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "099e9f2bc9c647d8a41f33c4632e3f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyaGanathas-novelit/Colaboratory/blob/main/Image_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUhwQZRpeJ3q",
        "outputId": "c1748df8-f14e-4be7-95b4-a096dd2a12ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "replace Flicker8k_Dataset/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace CrowdFlowerAnnotations.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip the downloaded files \n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7uhLuGKeN6_",
        "outputId": "8a65f471-5b39-4847-b7c2-0b9ea1ae477d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open Flickr8k_Dataset.zip, Flickr8k_Dataset.zip.zip or Flickr8k_Dataset.zip.ZIP.\n",
            "unzip:  cannot find or open Flickr8k_text.zip, Flickr8k_text.zip.zip or Flickr8k_text.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePEGYTesfxKX",
        "outputId": "b12d2db1-a002-406b-deaf-4dd600207a85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import h5py\n",
        "from PIL import Image as jpg\n",
        "import torch\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import XLNetTokenizer, XLNetModel\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "metadata": {
        "id": "V0XbkgGnePTG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = 1\n",
        "num_epoch = 25\n",
        "batch_size = 32\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dataset_root = os.path.join(\"/content\")\n",
        "kernel_root = os.path.join(\"/content\")"
      ],
      "metadata": {
        "id": "dLQJFG1beRwO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transformer = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        hidden = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask\n",
        "        ).last_hidden_state\n",
        "        context = hidden.mean(dim=1)\n",
        "        context = context.view(*context.shape, 1, 1)\n",
        "        return context"
      ],
      "metadata": {
        "id": "QQBvMMVofc_Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  class Generator(nn.Module):\n",
        "    def __init__(self, nz=100, nt=768, nc=3, ngf=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layer1 = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz + nt, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(ngf*8,ngf*2,1,1),\n",
        "            nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(ngf*2,ngf*2,3,1,1),\n",
        "            nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(ngf*2,ngf*8,3,1,1),\n",
        "            nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.SELU(True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),   \n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "        )\n",
        "\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(ngf*4,ngf,1,1),\n",
        "            nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "        )\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(ngf,ngf,3,1,1),\n",
        "            nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "        )\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(ngf,ngf*4,3,1,1),\n",
        "            nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "        )\n",
        "        self.layer9 = nn.Sequential(  \n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "            \n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "        )\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # nn.SELU(True),\n",
        "\n",
        "            # state size. (ngf) x 32 x 32\n",
        "        )\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "  \n",
        "    def forward(self,noise,encoded_text):\n",
        "        x = torch.cat([noise,encoded_text],dim=1)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.layer6(x)\n",
        "        x = self.layer7(x)\n",
        "        x = self.layer8(x)\n",
        "        x = self.layer9(x)\n",
        "        x = self.layer10(x)\n",
        "        x = self.layer11(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vlp1quAufdxe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc=3, ndf=64, nt=768):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "\n",
        "            nn.Conv2d(ndf*8,ndf*2,1,1),\n",
        "            # nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(ndf*2,ndf*2,3,1,1),\n",
        "            # nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.layer6 = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(ndf*2,ndf*8,3,1,1),\n",
        "            # nn.Dropout2d(inplace=True),            \n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.concat_image_n_text = nn.Sequential(\n",
        "            nn.Conv2d(ndf * 8 + nt, ndf * 8, 1, 1, 0, bias=False), \n",
        "            nn.BatchNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(ndf * 8, 2, 4, 1, 0, bias=False),\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, encoded_text):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.layer6(x)\n",
        "#         print(x.shape)\n",
        "        x = torch.cat([x, encoded_text.repeat(1, 1, 4, 4)], dim=1)\n",
        "        x = self.concat_image_n_text(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "G3E36U0eflT2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset:\n",
        "    def __init__(self, dataset_root, kernel_root):\n",
        "        self.tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
        "\n",
        "        image_path = os.path.join(dataset_root, \"Flicker8k_Dataset\")\n",
        "        text_path = os.path.join(kernel_root, \"Flickr8k.token.txt\")\n",
        "\n",
        "        if os.path.exists(os.path.join(kernel_root, \"data.npy\")):\n",
        "            self.data = np.load(os.path.join(kernel_root, \"data.npy\"), allow_pickle=True)\n",
        "        else:\n",
        "            self.data = self.prepareData(image_path, text_path)\n",
        "            np.save('data.npy', self.data)\n",
        "        self.max_seq_len = max(map(lambda x: len(x[\"text\"][\"input_ids\"]), self.data))\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(degrees=(90, 90)),\n",
        "            transforms.RandomRotation(degrees=(180, 180)),\n",
        "            transforms.RandomRotation(degrees=(270, 270)),\n",
        "            transforms.RandomVerticalFlip(p=1),\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def prepareData(self, image_path, text_path):\n",
        "        preparedData = []\n",
        "        with open(text_path, \"r\") as f:\n",
        "            captions = f.read().split(\"\\n\")\n",
        "\n",
        "        for caption in tqdm(captions):\n",
        "            if len(caption) == 0:\n",
        "                continue\n",
        "            parts = caption.split(\"\\t\")\n",
        "            image_name = parts[0]\n",
        "            caption_text = parts[1]\n",
        "            image_path = os.path.join(image_path, image_name)\n",
        "\n",
        "            image = np.array(Image.open(image_path).resize((256,256)))\n",
        "\n",
        "            input_ids = self.tokenizer.encode(caption_text)\n",
        "            token_type_ids = [0] * (len(input_ids) - 1) + [1]\n",
        "            attention_mask = [1] * len(token_type_ids)\n",
        "            preparedData.append({\n",
        "                \"image\": image,\n",
        "                \"text\": {\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"token_type_ids\": token_type_ids,\n",
        "                    \"attention_mask\": attention_mask\n",
        "                },\n",
        "            })\n",
        "        return preparedData\n",
        "\n",
        "    def padTokens(self, text_dict):\n",
        "        pad_len = self.max_seq_len - sum(text_dict[\"attention_mask\"])\n",
        "        text_dict['input_ids'] =  [5] * pad_len + text_dict['input_ids']\n",
        "        text_dict['token_type_ids'] =  [2] * pad_len + text_dict['token_type_ids']\n",
        "        text_dict['attention_mask'] = [0] * pad_len + text_dict['attention_mask']   \n",
        "        return text_dict\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn_module(batch, idx):\n",
        "        images, texts = [], {}\n",
        "        for data in batch:\n",
        "            images.append(data[idx][0])\n",
        "            for key in data[idx][1]:\n",
        "                if key not in texts:\n",
        "                    texts[key] = []\n",
        "                texts[key].append(data[0][1][key])\n",
        "\n",
        "        images = torch.stack(images).to(device)\n",
        "        for key in texts:\n",
        "            texts[key] = torch.tensor(texts[key]).to(device)\n",
        "        return images, texts\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        right_images, right_texts = self.collate_fn_module(batch, 0)\n",
        "        wrong_images, wrong_texts = self.collate_fn_module(batch, 1)\n",
        "        return (right_images, right_texts), (wrong_images, wrong_texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, right_idx):\n",
        "        right_data = self.data[right_idx].copy()\n",
        "        right_image = self.transforms(Image.fromarray(right_data[\"image\"]))\n",
        "        right_text = self.padTokens(right_data[\"text\"].copy())\n",
        "\n",
        "        wrong_idx = np.random.choice([(i) for i in range(len(self.data)) if i != right_idx])\n",
        "        wrong_data = self.data[wrong_idx].copy()\n",
        "        wrong_image = self.transforms(Image.fromarray(wrong_data[\"image\"]))\n",
        "        wrong_text = self.padTokens(wrong_data[\"text\"].copy())\n",
        "        return (right_image, right_text), (wrong_image, wrong_text)"
      ],
      "metadata": {
        "id": "67sKMV0QgOtw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_dataset = TrainDataset(dataset_root, kernel_root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447,
          "referenced_widgets": [
            "1bb44259db0048ef92da5449811c3833",
            "edb685fe518048e5ad0c9705959945f0",
            "5ab6c34240df40909ffc389d66e0f867",
            "81d78fb3f6174040866fe270fb78c4d7",
            "25af7f4a4a5144d59c03e6e9ba2ce780",
            "4d82ffef4d134a1fa5b9f3dffd20c64f",
            "175fa2f24d41419eab80d53bdd32a007",
            "b14271c87afe4b6a98f70a366cce8ac3",
            "73a1ee18e33a4023a067bc1feb7d7613",
            "818a3ac364a447dd8c955532f01cf9d0",
            "099e9f2bc9c647d8a41f33c4632e3f00"
          ]
        },
        "id": "Ds8FOI5OgRSN",
        "outputId": "777e6bf2-966c-4acf-a49b-d7b37d8e66b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/40461 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bb44259db0048ef92da5449811c3833"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a26614659ffc>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_root, kernel_root)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a26614659ffc>\u001b[0m in \u001b[0;36mprepareData\u001b[0;34m(self, image_path, text_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Flicker8k_Dataset/1000268201_693b08cb0e.jpg#0'"
          ]
        }
      ]
    }
  ]
}